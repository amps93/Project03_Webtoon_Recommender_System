{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979fda56-b61a-4cdf-b25e-4f2c794b368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "\n",
    "# URL = 'https://comic.naver.com/webtoon/weekday.nhn'\n",
    "URL = 'https://comic.naver.com/webtoon/finish'\n",
    "html = requests.get(URL).text # html 문서 전체를 긁어서 출력해줌, .text는 태그 제외하고 text만 출력되게 함\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "title = soup.find_all('a', {'class' : 'title'}) # a태그에서 class='title'인 html소스를 찾아 할당\n",
    "id_list = []\n",
    "title_list = []\n",
    "author_list = []\n",
    "day_list = []\n",
    "genre_list = []\n",
    "story_list = []\n",
    "score_list = []\n",
    "platform_list = []\n",
    "\n",
    "num = 0\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe') # 크롬 사용하니까\n",
    "driver.get(URL)\n",
    "\n",
    "for i in range(len(title)):\n",
    "    sleep(0.5) # 크롤링 중간 중간 텀을 주어 과부하 생기지 않도록\n",
    "\n",
    "    page = driver.find_elements_by_class_name('title')\n",
    "    page[i].click() #월요일 첫 번째 웹툰부터 순서대로 클릭\n",
    "\n",
    "    sleep(0.5)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser') # 이동한 페이지 주소 읽고 파싱\n",
    "\n",
    "    day = soup.find_all('ul', {'class' : 'category_tab'})\n",
    "    day = day[0].find('li', {'class' : 'on'}).text[0:1] # 요일 수집\n",
    "    \n",
    "    t = title[i].text\n",
    "    if (t in title_list):  # 요일 두 개 이상이면 요일만 추가함\n",
    "        day_list[title_list.index(t)] += ', ' + day\n",
    "        driver.back()\n",
    "        continue\n",
    "\n",
    "    id_list.append(num) ; num += 1  # id 리스트에 추가\n",
    "    title_list.append(t)  # 제목 리스트에 추가\n",
    "    day_list.append(day) # 요일 리스트에 추가\n",
    "    platform_list.append('네이버 웹툰') # 플랫폼 리스트에 추가\n",
    "\n",
    "    author = soup.find_all('h2') # 두 번째 h2태그에 있음\n",
    "    author = author[1].find('span', {'class' : 'wrt_nm'}).text[8:] # 7칸의 공백 후 8번 째부터 작가 이름임\n",
    "    author_list.append(author) # 작가 리스트에 추가\n",
    "\n",
    "    genre = soup.find('span', {'class' : 'genre'}).text # 장르 수집\n",
    "    genre_list.append(genre) # 장르 리스트에 추가\n",
    "\n",
    "    story = soup.find_all('p') # 줄거리 수집\n",
    "    story = str(story[3])\n",
    "    story = story.replace('<p>', '').replace('</p>', '').replace('<br/>', '\\n') # <br>을 개행으로 바꾸기\n",
    "    story_list.append(story) # 줄거리 리스트에 추가\n",
    "    \n",
    "    #최신 별점 평균 점수 수집 (최대 10화 분량)\n",
    "    score = soup.find_all('strong')\n",
    "    scorelist=[] ; ii=9\n",
    "    while score[ii].text[0].isnumeric()==True:\n",
    "        scorelist.append(float(score[ii].text))\n",
    "        ii +=1\n",
    "    score_list.append(sum(scorelist)/len(scorelist))\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "    driver.back() # 뒤로 가기\n",
    "\n",
    "############################################크롤링 끝############################################\n",
    "\n",
    "total_data = pd.DataFrame()\n",
    "total_data['id'] = id_list\n",
    "total_data['title'] = title_list\n",
    "total_data['author'] = author_list\n",
    "total_data['day'] = day_list\n",
    "total_data['genre'] = genre_list\n",
    "total_data['story'] = story_list\n",
    "total_data['score'] = score_list\n",
    "total_data['platform'] = platform_list\n",
    "total_data.to_csv('네이버_웹툰.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ae99a-0574-4e7b-ba0e-90dbda667829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('네이버_웹툰.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2abd1-745b-4dc5-91a4-dd841c84707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence_kr(w):\n",
    "  w = w.strip()\n",
    "  w = re.sub(r\"[^0-9가-힣?.!,¿]+\", \" \", w) # \\n도 공백으로 대체해줌\n",
    "  w = w.strip()\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada3ac4-7e9c-408c-8b15-75575276abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['story'] = [preprocess_sentence_kr(l) for l in df['story']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf771ba-563e-4871-85a9-635f3de5e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef3c84b-aa5b-4b4a-b7ac-05597762f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('NAVER.csv', encoding='euc-kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d4d8e3-ed1d-4b21-963c-4b42f6d46beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mu",
   "language": "python",
   "name": "mu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
